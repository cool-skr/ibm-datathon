{
 "cells": [
  {
   "cell_type": "raw",
   "id": "dcb58893-3868-4497-8623-8fe5aebb2d2e",
   "metadata": {},
   "source": [
    "THE BELOW CODE IS USING A PRE-TRAINED MODEL \"FACEBOOK/BART-LARGE-CNN\", WHICH IS A VARIANT OF THE BART. IT IS QUITE HEAVY BECAUSE IT INVOLVES IMPORTING THE MODEL AND USING IT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f3fca-8bf2-4717-ba3e-139eef142bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained text simplification model\n",
    "simplifier = pipeline(\"text2text-generation\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Input text to simplify\n",
    "text = \"The comprehensive analysis of the experimental data led to new scientific insights.\"\n",
    "\n",
    "# Perform text simplification\n",
    "simplified_text = simplifier(text, max_length=50, min_length=10, do_sample=False)[0]['generated_text']\n",
    "\n",
    "print(\"Simplified Text:\", simplified_text)'''\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c06ad4d9-ff4f-4151-a235-565ce87b7cd8",
   "metadata": {},
   "source": [
    "CODE TO TRANSLATE TEXT IN ANY LANGUAGE TO ENGLISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "904f79ca-59b0-4606-82a4-16ebdf5ec78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f7735fbbbe410aa4110a9dad722f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683568bb955d4981b3794d8a60434901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d59b35e03ef4795b0cd327efc4515f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eeb2f538d2d40eba11e98e32327456f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48059d981a5e4b84ba9aad90a40e33f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "/home/jovyan/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf8a5ec03c8483ab5d46b1352875f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7465c8bf8e2481ebf4a55526f78590c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in cpuinfo: processor architecture is not supported in cpuinfo\n",
      "Error in cpuinfo: processor architecture is not supported in cpuinfo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Je suis heureux de vous rencontrer.\n",
      "Translated text: I'm glad to meet you.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Function to translate text to English\n",
    "def translate_to_english(text, source_language_code):\n",
    "    # Load the model and tokenizer for translating from the source language to English\n",
    "    model_name = f'Helsinki-NLP/opus-mt-{source_language_code}-en'\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize the text and generate translation\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    translated = model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "    return translated_text\n",
    "\n",
    "# Example usage\n",
    "text = \"Je suis heureux de vous rencontrer.\"\n",
    "source_language_code = 'fr'  # French\n",
    "\n",
    "translated_text = translate_to_english(text, source_language_code)\n",
    "print(\"Original text:\", text)\n",
    "print(\"Translated text:\", translated_text)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "72bc6423-43f9-4146-9f3e-52485722df35",
   "metadata": {},
   "source": [
    "CODE TO CONVERT VOICE RECORDING TO TEXT IN ENGLISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84038ab3-af04-4ce5-bfe6-78c1556ffa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting SpeechRecognition\n",
      "  Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.local/lib/python3.10/site-packages (from SpeechRecognition) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions in ./.local/lib/python3.10/site-packages (from SpeechRecognition) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2022.12.7)\n",
      "Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl (32.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub, SpeechRecognition\n",
      "Successfully installed SpeechRecognition-3.10.4 pydub-0.25.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install SpeechRecognition pydub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9643660a-0b06-4ac6-a944-c0a9e6979b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# convert audio file to English text\n",
    "def audio_to_text(audio_file_path):\n",
    "    # Initialize the recognizer\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    # Load the audio file \n",
    "    audio = AudioSegment.from_file(audio_file_path)\n",
    "\n",
    "    # Export the audio to .wav format\n",
    "    audio.export(\"temp.wav\", format=\"wav\")\n",
    "\n",
    "    # Use SpeechRecognition to read the .wav file\n",
    "    with sr.AudioFile(\"temp.wav\") as source:\n",
    "        audio_data = recognizer.record(source)  # Read the entire audio file\n",
    "\n",
    "        try:\n",
    "            # Recognize speech using Google Web Speech API and specify 'en-US' for English\n",
    "            text = recognizer.recognize_google(audio_data, language=\"en-US\")\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            return \"Google Speech Recognition could not understand the audio.\"\n",
    "        except sr.RequestError as e:\n",
    "            return f\"Could not request results from Google Speech Recognition service; {e}\"\n",
    "\n",
    "# Example usage\n",
    "audio_file_path = \"voice_recording.mp3\"  \n",
    "english_text = audio_to_text(audio_file_path)\n",
    "print(\"Transcribed English text:\", english_text)   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
