{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abe11c3e-99e8-47a8-a23e-af5396c3584f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x3ffad636bc0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2079c48d275948ec978be20d889cf315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb1b89781844ad39f90f7114d03e6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ace97590ef4b448ad36fb4d6dba38e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba4c067917c4cc2b863cc1a6a759b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ec5a82cb12494b80aa4def7fb6ae8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2db9ddd2de456e86eaa8200b750d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Error in cpuinfo: processor architecture is not supported in cpuinfo\n",
      "Error in cpuinfo: processor architecture is not supported in cpuinfo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 13.400545120239258\n",
      "Epoch 2/3, Loss: 16.733417510986328\n",
      "Epoch 3/3, Loss: 15.273792266845703\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
    "\n",
    "# Define your dataset class\n",
    "class SimplificationDataset(Dataset):\n",
    "    def __init__(self, complex_texts, simplified_texts, tokenizer, max_len=128):\n",
    "        self.complex_texts = complex_texts\n",
    "        self.simplified_texts = simplified_texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.complex_texts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        complex_text = self.complex_texts[index]\n",
    "        simplified_text = self.simplified_texts[index]\n",
    "        \n",
    "        # Tokenize inputs and labels\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            complex_text, \n",
    "            max_length=self.max_len, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = self.tokenizer.encode_plus(\n",
    "            simplified_text, \n",
    "            max_length=self.max_len, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        label_ids = labels['input_ids'].squeeze()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label_ids\n",
    "        }\n",
    "\n",
    "# Function to load dataset (example for LexMTurk)\n",
    "def load_lexmturk_data():\n",
    "    # Load LexMTurk dataset (complex and simplified sentences)\n",
    "    # Example pairs; in practice, load from dataset files\n",
    "    complex_sentences = [\n",
    "        \"The researcher extrapolated a comprehensive analysis of the convoluted data.\"\n",
    "    ]\n",
    "    simplified_sentences = [\n",
    "        \"The researcher made a thorough analysis of the complex data.\"\n",
    "    ]\n",
    "    return complex_sentences, simplified_sentences\n",
    "\n",
    "# Fine-tuning function\n",
    "def fine_tune_t5(model, tokenizer, dataset, epochs=3, batch_size=8, learning_rate=1e-4):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Use AdamW optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader)}\")\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load a pre-trained T5 model and tokenizer\n",
    "    model_name = \"t5-small\"  # Can choose \"t5-base\" or \"t5-large\" for larger models\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "    # Load the dataset\n",
    "    complex_texts, simplified_texts = load_lexmturk_data()\n",
    "    dataset = SimplificationDataset(complex_texts, simplified_texts, tokenizer)\n",
    "    \n",
    "    # Fine-tune the model\n",
    "    fine_tune_t5(model, tokenizer, dataset, epochs=3, batch_size=4, learning_rate=1e-4)\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained('./fine_tuned_t5')\n",
    "    tokenizer.save_pretrained('./fine_tuned_t5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee039a1b-1cec-491e-990f-10b0ef8b1dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c83dd05-bb2b-46a4-a600-a855d508f6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helloe\n"
     ]
    }
   ],
   "source": [
    "print(\"Helloe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61cb79a6-9ce6-41b7-82ba-d277a92e17ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
    "\n",
    "# Define your dataset class\n",
    "class SimplificationDataset(Dataset):\n",
    "    def __init__(self, complex_texts, simplified_texts, tokenizer, max_len=128):\n",
    "        self.complex_texts = complex_texts\n",
    "        self.simplified_texts = simplified_texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.complex_texts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        complex_text = self.complex_texts[index]\n",
    "        simplified_text = self.simplified_texts[index]\n",
    "        \n",
    "        # Tokenize inputs and labels\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            complex_text, \n",
    "            max_length=self.max_len, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = self.tokenizer.encode_plus(\n",
    "            simplified_text, \n",
    "            max_length=self.max_len, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        label_ids = labels['input_ids'].squeeze()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label_ids\n",
    "        }\n",
    "\n",
    "# Function to load BenchLS dataset from file\n",
    "def load_benchls_data(file_path):\n",
    "    complex_sentences = []\n",
    "    simplified_sentences = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Each line contains a complex sentence and simplified sentence separated by a tab\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                complex_sentences.append(parts[0])\n",
    "                simplified_sentences.append(parts[1])\n",
    "    \n",
    "    return complex_sentences, simplified_sentences\n",
    "\n",
    "# Fine-tuning function\n",
    "def fine_tune_t5(model, tokenizer, dataset, epochs=3, batch_size=8, learning_rate=1e-4):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Use AdamW optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fb2ebdc-a523-494b-97ea-012ce4a3d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load a pre-trained T5 model and tokenizer\n",
    "model_name = \"t5-small\"  # Can choose \"t5-base\" or \"t5-large\" for larger models\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ebc505c-497c-4232-b02c-e505824643a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BenchLS dataset from the text file\n",
    "file_path = 'BenchLS.txt'  # Path to your BenchLS.txt file\n",
    "complex_texts, simplified_texts = load_benchls_data(file_path)\n",
    "\n",
    "# Prepare the dataset\n",
    "dataset = SimplificationDataset(complex_texts, simplified_texts, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b90d2529-71f8-46aa-a6f5-94ed8225666e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mfine_tune_t5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./fine_tuned_t5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 63\u001b[0m, in \u001b[0;36mfine_tune_t5\u001b[0;34m(model, tokenizer, dataset, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfine_tune_t5\u001b[39m(model, tokenizer, dataset, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Use AdamW optimizer\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:349\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 349\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    351\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/sampler.py:140\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "fine_tune_t5(model, tokenizer, dataset, epochs=3, batch_size=4, learning_rate=1e-4)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_t5')\n",
    "tokenizer.save_pretrained('./fine_tuned_t5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e552c9d6-874e-4c06-bdf0-4738a4c41a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c8ca005-d346-4f33-8e4b-007d6453a99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 sentence pairs from BenchLS.txt\n"
     ]
    }
   ],
   "source": [
    "complex_texts, simplified_texts = load_benchls_data(file_path)\n",
    "print(f\"Loaded {len(complex_texts)} sentence pairs from BenchLS.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0dccbfd-0cdf-470e-a784-997d3bd18542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
    "\n",
    "# Dataset class that extracts sentences and replaces the complex word with a synonym\n",
    "class BenchLSDataset(Dataset):\n",
    "    def __init__(self, sentences, complex_words, simplified_words, tokenizer, max_len=128):\n",
    "        self.sentences = sentences\n",
    "        self.complex_words = complex_words\n",
    "        self.simplified_words = simplified_words\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.sentences[index]\n",
    "        complex_word = self.complex_words[index]\n",
    "        simplified_word = self.simplified_words[index]\n",
    "        \n",
    "        # Replace the complex word in the sentence with the simplified word\n",
    "        simplified_sentence = sentence.replace(complex_word, simplified_word)\n",
    "        \n",
    "        # Tokenize inputs and labels\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            sentence, \n",
    "            max_length=self.max_len, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = self.tokenizer.encode_plus(\n",
    "            simplified_sentence, \n",
    "            max_length=self.max_len, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        label_ids = labels['input_ids'].squeeze()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label_ids\n",
    "        }\n",
    "\n",
    "# Function to load BenchLS dataset from file\n",
    "def load_benchls_data(file_path):\n",
    "    sentences = []\n",
    "    complex_words = []\n",
    "    simplified_words = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 5:  # Ensure correct number of fields\n",
    "                sentence = parts[0]\n",
    "                complex_word = parts[1]\n",
    "                # Extract the best possible simplified synonym (in this case, use the first synonym as an example)\n",
    "                synonyms = parts[3:]  # synonyms are listed after the complex word and position\n",
    "                best_synonym = synonyms[0].split(\":\")[1]  # Get the first synonym\n",
    "                \n",
    "                sentences.append(sentence)\n",
    "                complex_words.append(complex_word)\n",
    "                simplified_words.append(best_synonym)\n",
    "            else:\n",
    "                print(f\"Skipping invalid line: {line}\")\n",
    "    \n",
    "    return sentences, complex_words, simplified_words\n",
    "\n",
    "# Fine-tuning function remains the same\n",
    "def fine_tune_t5(model, tokenizer, dataset, epochs=3, batch_size=8, learning_rate=1e-4):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Use AdamW optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bdb90d-d7c8-44f5-a9ee-158045b132a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid line: they locate food by smell , using sensors in the tip of their snout , and regularly feast on ants and termites .\tsnout\t13\t1:nose\n",
      "\n",
      "Skipping invalid line: the latter means basic or radical change ; whereas reform may be no more than fine tuning , or at most redressing serious wrongs without altering the fundamentals of the system .\taltering\t25\t1:changing\n",
      "\n",
      "Skipping invalid line: the band is known for its large line-up , which consists of nine members ; including a vocalist , two guitarists , a bassist , two percussionists in addition to a primary drummer , a sampler , and a turntablist .\tvocalist\t17\t1:singer\n",
      "\n",
      "Skipping invalid line: during the final immunity challenge , Sandra fell out early , allowing Jon to try to convince Lil to take him to the final two .\tfinal\t23\t1:last\n",
      "\n",
      "Skipping invalid line: Italy purchased the city in 1905 and made Mogadishu the capital of Italian Somaliland .\tpurchased\t1\t1:bought\n",
      "\n",
      "Skipping invalid line: he aided the Doukhobors in migrating to Canada .\taided\t1\t1:helped\n",
      "\n",
      "Skipping invalid line: men are supposed to come to the mosque wearing loose and clean clothes that do not reveal the shape of the body .\treveal\t16\t1:show\n",
      "\n",
      "Skipping invalid line: it is currently used mostly for football matches and is the home stadium of Union sportive du foyer de la Rgie Abidjan-Niger .\tmatches\t7\t1:games\n",
      "\n",
      "Skipping invalid line: the family - mother , father , old woman , young man , baby , and dog - are attempting to eat pieces of metal , such as chains , bicycle handlebars , and rifles .\tattempting\t19\t1:trying\n",
      "\n",
      "Skipping invalid line: Galls indeed arise from the stinging of the plant tissues by the ovipositors of female gall wasps , and the egg laid in the plant tissues develops inside the gall into a grub , which eventually emerges full-grown and transformed into a mature gall wasp .\tgall\t43\t2:pn\n",
      "\n",
      "Skipping invalid line: these include laparoscopic gall bladder surgery and reflux disease surgery , non-cardiac thoracoscopic chest surgery , and certain cardiotomy procedures such as mitral valve repair .\tgall\t3\t2:pn\n",
      "\n",
      "Skipping invalid line: Galls indeed arise from the stinging of the plant tissues by the ovipositors of female gall wasps , and the egg laid in the plant tissues develops inside the gall into a grub , which eventually emerges full-grown and transformed into a mature gall wasp .\tgalls\t0\t1:lists\n",
      "\n",
      "Skipping invalid line: prevalence of different agro-ecological tracts has made it possible to grow a multitude of over 60 crops ranging from typical tropical ones to moderate temperate varieties including cereals such as rice , wheat . maize , millet , beans , pulses and oilseeds : industrial crops such as cotton , jute , rubber , sugar cane , toddy palm , tobacco and spices , and many others both edible and non-edible .\tpulses\t40\t1:legumes\n",
      "\n",
      "Skipping invalid line: it 's also present in adequate quantities in dairy products , eggs and nuts , as well as in combinations of foods such as pulses and grains .\tpulses\t24\t1:legumes\n",
      "\n",
      "Skipping invalid line: the remainder of the soup list evokes eastern Europe , comprising soljanka , a ruddy vegetable-based soup containing a significant fraction of some unfortunate animal ( $ 4.50 ) and a sweetish roasted beet soup ( $ 4 ) .\tremainder\t1\t1:rest\n",
      "\n",
      "Skipping invalid line: however , with only .14 of an inch falling the remainder Of the month , this left only 2.86 for the month , or .69 of an inch below normal .\tremainder\t10\t1:rest\n",
      "\n",
      "Skipping invalid line: maybe a journalist 's privilege should likewise be limited .\tlikewise\t6\t1:similarly\n",
      "\n",
      "Skipping invalid line: we are also reminding the Member States that they should do likewise .\tlikewise\t11\t1:similarly\n",
      "\n",
      "Skipping invalid line: your trailer or caravan must be fitted with an approved style number plate .\tcaravan\t3\t1:camper\n",
      "\n",
      "Skipping invalid line: this external communication could be with external trading partners or within the organization .\texternal\t1\t1:outside\n",
      "\n",
      "Skipping invalid line: that is , we can be glad so long as we do not believe in external authority ( for example , the testimony of others ) alone for our religious beliefs .\texternal\t15\t1:outside\n",
      "\n",
      "Skipping invalid line: Q Can you kindly advise if Team roles change with different team members and external factors e.g . company setting , cultural context ?\texternal\t14\t1:outside\n",
      "\n",
      "Skipping invalid line: suspected of conspiracy , securities fraud and obstruction of justice , Stewart is ultimately indicted , found guilty of all charges and sentenced to five months in prison followed by five months of house arrest .\tsuspected\t0\t2:mistrusted\n",
      "\n",
      "Skipping invalid line: Cook the rice , covered , over boiling water for about 30 minutes , or until the grains are tender .\ttender\t19\t1:soft\n",
      "\n",
      "Skipping invalid line: but a new survey shows that the execution of Ms. Tucker and the resulting debate led some residents of the Lone Star State to have second thoughts about capital punishment .\texecution\t7\t1:killing\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 1.0408601090971348\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load a pre-trained T5 model and tokenizer\n",
    "model_name = \"t5-small\"  # Can choose \"t5-base\" or \"t5-large\" for larger models\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Load the BenchLS dataset from the text file\n",
    "file_path = 'BenchLS.txt'  # Path to your BenchLS.txt file\n",
    "sentences, complex_words, simplified_words = load_benchls_data(file_path)\n",
    "\n",
    "# Prepare the dataset\n",
    "dataset = BenchLSDataset(sentences, complex_words, simplified_words, tokenizer)\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune_t5(model, tokenizer, dataset, epochs=3, batch_size=4, learning_rate=1e-4)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_t5')\n",
    "tokenizer.save_pretrained('./fine_tuned_t5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e4fd4a6-a2e0-4be8-a0c4-665be51a38c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in cpuinfo: processor architecture is not supported in cpuinfo\n",
      "Error in cpuinfo: processor architecture is not supported in cpuinfo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Escapologists escape from handcuffs, straitjackets, coffins, and other perils.\n",
      "Simplified text: simplify: Escapologists escape from handcuffs, straitjackets, coffins, and other perils.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Function to generate predictions using the fine-tuned model\n",
    "def simplify_text(model, tokenizer, text, max_len=128):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Prepare the input\n",
    "    input_text = f\"simplify: {text}\"\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        input_text, \n",
    "        max_length=max_len, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            max_length=max_len,\n",
    "            num_beams=4,  # You can adjust the number of beams for better quality\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    simplified_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return simplified_text\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = './fine_tuned_t5'  # Path to the fine-tuned model\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "\n",
    "# Example to test the model\n",
    "if __name__ == \"__main__\":\n",
    "    # Original text containing complex words\n",
    "    original_text = \"Escapologists escape from handcuffs, straitjackets, coffins, and other perils.\"\n",
    "    \n",
    "    # Get the simplified text using the fine-tuned model\n",
    "    simplified_text = simplify_text(model, tokenizer, original_text)\n",
    "    \n",
    "    print(f\"Original text: {original_text}\")\n",
    "    print(f\"Simplified text: {simplified_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1d978b-9f7b-40d1-8e28-e8501810a3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b088d415-d2f3-4ef0-92c9-e9172232508f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:51<00:00,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match Accuracy: 0.00%\n",
      "Average BLEU Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "# Function to simplify text using the fine-tuned model\n",
    "def simplify_text(model, tokenizer, text, max_len=128):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Prepare the input\n",
    "    input_text = f\"simplify: {text}\"\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        input_text,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_len,\n",
    "            num_beams=4,  # You can adjust the number of beams for better quality\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    # Decode the generated text\n",
    "    simplified_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return simplified_text\n",
    "\n",
    "# Function to calculate accuracy and BLEU score\n",
    "def evaluate_model(model, tokenizer, sentences, complex_words, simplified_words):\n",
    "    exact_match_count = 0\n",
    "    bleu_scores = []\n",
    "    smoothing_function = SmoothingFunction().method1  # Smoothing for BLEU score\n",
    "    c=0\n",
    "    for i in tqdm(range(25)):\n",
    "        # c+=1\n",
    "        # if c==20:\n",
    "        #     break\n",
    "        original_sentence = sentences[i]\n",
    "        correct_simplified_word = simplified_words[i]  # Ground truth simplified word\n",
    "        complex_word = complex_words[i]\n",
    "\n",
    "        # Simplify the sentence using the model\n",
    "        predicted_sentence = simplify_text(model, tokenizer, original_sentence)\n",
    "\n",
    "        # Extract the simplified word from the predicted sentence\n",
    "        # (Assuming we replaced the complex word in the sentence)\n",
    "        if complex_word in original_sentence:\n",
    "            predicted_simplified_word = predicted_sentence.replace(original_sentence.replace(complex_word, ''), '').strip()\n",
    "        else:\n",
    "            predicted_simplified_word = predicted_sentence\n",
    "\n",
    "        # Check for exact match\n",
    "        if predicted_simplified_word == correct_simplified_word:\n",
    "            exact_match_count += 1\n",
    "\n",
    "        # Compute BLEU score\n",
    "        reference = [correct_simplified_word.split()]  # Ground truth as reference\n",
    "        candidate = predicted_simplified_word.split()  # Predicted as candidate\n",
    "        bleu_score = sentence_bleu(reference, candidate, smoothing_function=smoothing_function)\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "    # Calculate exact match accuracy\n",
    "    exact_match_accuracy = exact_match_count / len(sentences)\n",
    "\n",
    "    # Calculate average BLEU score\n",
    "    average_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "    return exact_match_accuracy, average_bleu_score\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = './fine_tuned_t5'  # Path to the fine-tuned model\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "\n",
    "# Load the dataset for testing\n",
    "file_path = 'BenchLS.txt'  # Path to your BenchLS.txt file\n",
    "sentences, complex_words, simplified_words = load_benchls_data(file_path)\n",
    "\n",
    "# Evaluate the model\n",
    "exact_match_accuracy, average_bleu_score = evaluate_model(model, tokenizer, sentences, complex_words, simplified_words)\n",
    "\n",
    "print(f\"Exact Match Accuracy: {exact_match_accuracy * 100:.2f}%\")\n",
    "print(f\"Average BLEU Score: {average_bleu_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "650d9d97-b0af-4a37-a3e3-ceabf7fd8ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_benchls_data(file_path):\n",
    "    sentences = []\n",
    "    complex_words = []\n",
    "    simplified_words = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            \n",
    "            # Ensure the line has at least 3 parts: sentence, complex word, and simplified options\n",
    "            if len(parts) >= 3:\n",
    "                sentence = parts[0]\n",
    "                complex_word = parts[1]\n",
    "                \n",
    "                # Get the first simplified word as ground truth (you can modify this if needed)\n",
    "                simplified_word = parts[3].split(':')[1]  # Taking the first simplified word\n",
    "                \n",
    "                sentences.append(sentence)\n",
    "                complex_words.append(complex_word)\n",
    "                simplified_words.append(simplified_word)\n",
    "\n",
    "    return sentences, complex_words, simplified_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc606a0-95e3-4d5e-b9b7-ca5641e0153e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
